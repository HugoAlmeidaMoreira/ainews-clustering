
--- Page 1 ---
Clustering (1/2)
Introduction to clustering
DASH: Data Science e AnÃ¡lise NÃ£o Supervisionada
Rui Henriques, rmch@tecnico.ulisboa.pt 
Instituto Superior TÃ©cnico, Universidade de Lisboa
--- Page 2 ---
2
Outline
â€¢ Introduction to clustering
â€¢ Multivariate similarity metrics
â€¢ Approaches
â€¢ hierarchical
â€¢ density-based
â€¢ From multivariate to complex data structures
â€¢ Evaluation
â€¢ intrinsic metrics
â€¢ extrinsic metrics
--- Page 3 ---
3
Clustering
Cluster: group of observations
Cluster analysis: group observations into clusters according to their (dis)similarity: 
observations in the same cluster are more similar than those in different clusters
Intercluster 
distances are 
maximized
Intracluster 
distances are 
minimized
--- Page 4 ---
4
Motivation
â€¢ Patients with a shared clinical condition:                                                                                  
How to understand disease? 
â€¢ cancer types, dementia progression, risk groups
â€¢ stratified diagnostics and therapeutics 
â€¢ Customers: how to segment their profile for personalized marketing?
â€¢ Webpages, shopping products, media, documents:                                                           
how to categorize them for recommendations?
â€¢ Genes, proteins and metabolites with different expression and concentration profile: how to 
understand their correlated behavior (biological functions)?
â€¢ Students, researchers, professors: how to improve science and education?

--- Page 5 ---
5
Motivation
â€¢ ENDS
â€¢ Insight into the underlying structure/regularities of data
â€¢ Preprocessing step for other tasks
â€¢ Supporting prediction by stratifying populations (exercise: how?)
â€¢ Improving efficiency by using clusters as a proxy for observations
â€¢ Many othersâ€¦
â€¢ Application DOMAINS
â€¢ Information retrieval: document and webpage clustering
â€¢ Marketing: customer groups according to profile and product-receptivity
â€¢ Insurance: policy holders with different average claim costs
â€¢ Medicine: risk groups, personalized medicine
â€¢ Biology: philogenetics, pathways, regulatory modules
â€¢ Others: city-planning, land use, seismic studies, atmospheric conditions
--- Page 6 ---
6
Illustration
--- Page 7 ---
7
Clustering modes
â€¢ Unsupervised (default)
â€¢ cluster observations without knowning their labels
â€¢ Semi-supervised
â€¢ cluster observations when:
â€¢ the labels of some observations may be known or 
â€¢ pairs of observations are known to belong to the same cluster
â€¢ Supervised
â€¢ cluster observations when targets are considered, e.g.:
â€¢ label added as an additional input variable
â€¢ cluster class-conditional observations
--- Page 8 ---
8
Clustering modes
â€¢ Deterministic versus probabilistic cluster stances
â€¢ hard solutions: each observation either belongs or not to a given cluster 
â€¢ soft solutions: each observation has a probability (membership) of belonging 
to a given cluster
â€¢ fuzzy and model-based clustering
â€¢ Separation of clusters: exclusive versus non-exclusive (overlapping clusters)
â€¢ Complete versus partial (observations may not belong to any cluster)
â€¢ Uniform versus weighted
â€¢ variables can be weighted based on data semantics/domain knowledge
â€¢ observations can be weighted based on relevance criteria
--- Page 9 ---
9
Motivation
Two major factors impact solutions: distance + approach
â€¢ distance metrics depend on the: 
â€¢ variable domains
â€¢ numeric and ordinal (e.g. Euclidean)
â€¢ nominal (e.g. Hamming)
â€¢ non-iid attributes
â€¢ data structure: tabular, time series, image, spatiotemporal data, eventsâ€¦
â€¢ approach
â€¢ partitioning
â€¢ hierarchical
â€¢ density-based
â€¢ model-based 
--- Page 10 ---
10
Outline
â€¢ Introduction to clustering
â€¢ Multivariate similarity metrics
â€¢ Approaches
â€¢ hierarchical
â€¢ density-based
â€¢ From multivariate to complex data structures
â€¢ Evaluation
â€¢ intrinsic metrics
â€¢ extrinsic metrics
--- Page 11 ---
11
â€¢ well-established distances can be applied yetâ€¦                                                                      
   â€¦best distances are generally customized to the problem domain (background knowledge)
â€¢ e.g. demographic dist ğ‘–ğ‘›ğ‘‘1, ğ‘–ğ‘›ğ‘‘2 = ğ‘ğ‘”ğ‘’1âˆ’ğ‘ğ‘”ğ‘’2
20 + ğŸ ğ‘Ÿğ‘’ğ‘”ğ‘–ğ‘œğ‘›1 = ğ‘Ÿğ‘’ğ‘”ğ‘–ğ‘œğ‘›2 Ã— 0.8 + 1 ğ‘ ğ‘’ğ‘¥1 = ğ‘ ğ‘’ğ‘¥2 Ã— 1.2 + â‹¯
â€¢ apply distance to produce pairwise distance matrices between observations (and/or clusters)
â€¢ similarity matrix = âˆ’ distance matrix
Focal point: distances
A B C D E F
A 0 0.71 5.66 3.61 4.24 3.20
B 0.71 0 4.95 2.92 3.54 2.50
C 5.66 4.95 0 2.24 1.41 2.50
D 3.61 2.92 2.24 0 1.00 0.50
E 4.24 3.54 1.41 1.00 0 1.12
F 3.20 2.50 2.50 0.50 1.12 0
--- Page 12 ---
12
Clustering as a graph-based task
â€¢ Proximity between all data observations defines a weighted graph
â€¢ Nodes are the observations, edges capture their distances
â€¢ Clustering = breaking the graph into connected components
â€¢ Minimize the edge weight between clusters AND maximize the edge weight within clusters 
â€¢ How? Incremental grouping using thresholds
A B C D E
A 0 1 2 2 3
B 1 0 2 4 3
C 2 2 0 1 5
D 2 4 1 0 3
E 3 3 5 3 0 A B C D E
BA
E C
D
--- Page 13 ---
13
A distance function is a metric if the following conditions are met:
â€¢ non-negative
d(x, y) â‰¥ 0
â€¢ distance to point itself is zero
d(x, x) = 0
â€¢ symmetry
d(x, y) = d(y, x)
â€¢ triangular inequality
d(x, y) ï‚£ d(x, z) + d(z, y)
Distances and metrics

--- Page 14 ---
14
Common distance metrics
(numeric data)
Minkowski distance
 Euclidean distance (ğ‘ = 2)
 Manhattan distance (ğ‘ = 1)
ğ‘‘(ğ‘–, ğ‘—) = ğ‘ğ‘–1 âˆ’ ğ‘ğ‘—1
2
+ ğ‘ğ‘–2 âˆ’ ğ‘ğ‘—2
2
+. . . + ğ‘ğ‘–ğ‘ âˆ’ ğ‘ğ‘—ğ‘
2
ğ±ğ‘– =  (ğ‘ğ‘–1, ğ‘ğ‘–2, â€¦ , ğ‘ğ‘–ğ‘)
dij = ?
ğ±ğ‘— =  (ğ‘ğ‘—1, ğ‘ğ‘—2, â€¦ , ğ‘ğ‘—ğ‘)
ğ‘‘(ğ‘–, ğ‘—) = ğ‘ğ‘–1 âˆ’ ğ‘ğ‘—1 + ğ‘ğ‘–2 âˆ’ ğ‘ğ‘—2 +. . . + ğ‘ğ‘–ğ‘ âˆ’ ğ‘ğ‘—ğ‘
ğ‘‘(ğ‘–, ğ‘—) =
ğ‘
ğ‘ğ‘–1 âˆ’ ğ‘ğ‘—1
ğ‘
+ ğ‘ğ‘–2 âˆ’ ğ‘ğ‘—2
ğ‘
+. . . + ğ‘ğ‘–ğ‘ âˆ’ ğ‘ğ‘—ğ‘
ğ‘
1st dimension 2nd dimension pth dimension
--- Page 15 ---
15
Common distance metrics
(numeric data)
2D example
 x1 = (2,8)
 x2 = (6,3)
Euclidean distance
Manhattan distance
0 5 1
0
5
1
0
4
5
X1 = (2,8)
X2 = (6,3)
413862)2,1(
22
=âˆ’+âˆ’=d
93862)2,1( =âˆ’+âˆ’=d
--- Page 16 ---
16
ğ‘‘âˆ(ğ±, ğ²) = lim
ğ‘â†’âˆ
à·
ğ‘–=1
ğ‘›
ğ‘¥ğ‘– âˆ’ ğ‘¦ğ‘– ğ‘
àµ—1 ğ‘
= max ğ‘¥1 âˆ’ ğ‘¦1 , ğ‘¥2 âˆ’ ğ‘¦2 , â€¦ , ğ‘¥ğ‘› âˆ’ ğ‘¦ğ‘›
â€¢ when q â†’ ï‚¥, the metric highly penalizes maximum attribute errors
â€¢ useful if the worst case must be avoided:
ğ‘‘âˆ (2,8), (6,3) = max 2 âˆ’ 6 , 8 âˆ’ 3 = max 4,5 = 5
Example:
Chebyshev distance                                             
(numeric data)
--- Page 17 ---
17
Correlation
â€¢ positive (negative): two variables vary in the same (opposite) way
â€¢ maximum value of 1 (-1) if X and Y are perfectly direct (inverse) correlated
â€¢ recall: Pearson and Spearman coefficients for numeric data
â€¢ how to handle categorical or mixed data?
â€¢ example: gene expression data clustering
g1 = (1,2,3,4,5)
g2 = (100,200,300,400,500)
g3 = (5,4,3,2,1)
Which genes are similar according to correlation coefficients? 
--- Page 18 ---
18
â€¢ number of different attribute values
â€¢ distance of (1011101) and (1001001) is 2
â€¢ distance between (toned) and (roses) is 3 
Hamming distance
(binary and categorical data)
3-bit binary cube 100->011 has distance 3 (red path) 
010->111 has distance 2 (blue path)
--- Page 19 ---
19
Outline
â€¢ Introduction to clustering
â€¢ Multivariate similarity metrics
â€¢ Approaches
â€¢ hierarchical
â€¢ density-based
â€¢ From multivariate to complex data structures
â€¢ Evaluation
â€¢ intrinsic metrics
â€¢ extrinsic metrics
--- Page 20 ---
20
Approaches
Partitioning:
â€¢ Create partitions and iteratively update them 
(e.g. k-means, k-modes, k-medoids)
Hierarchical: 
â€¢ Create hierarchical decomposition of data points 
(e.g. Diana, Agnes)
Density-based: 
â€¢ Group points based on connectivity and density                   
(e.g. DBSACN, DenClue)
Model-based: 
â€¢ Data are seen as a mixture of distributions (e.g. EM)

--- Page 21 ---
21
Step 0 Step 1 Step 2 Step 3 Step 4
b
d
c
e
a
a b
d e
c d e
a b c d e
Step 4 Step 3 Step 2 Step 1 Step 0
agglomerative
(e.g. AGNES)
divisive
(e.g. DIANA)
Hierarchical clustering
--- Page 22 ---
22
â€¢ Agglomerative (bottom-up)
â€¢ initialize each point as its own cluster
â€¢ iteratively merge clusters
â€¢ Divisive (top-down)
â€¢ initialize all data points into one cluster
â€¢ large clusters are successively divided
Hierarchical clustering
--- Page 23 ---
23
The number of dendrograms with n leafs  = (2n -3)!/[(2(n -2)) (n -2)!]
Number Number of Possible
of Leafs Dendrograms 
2  1
3  3
4  15
5  105
...  â€¦
10  34,459,425
cannot test all possible trees
     âŸ¹ heuristic searches
Hierarchical clustering
--- Page 24 ---
24
Cluster distance
â€¢ Single link: smallest distance between observations
â€¢ Complete link: largest distance between observations
â€¢ Average link: average distance between observations
â€¢ Centroid link: distance between centroids
â€¢ Wardâ€™s distance: similarity based on the error increase when two clusters are merged           
(sum of squared distances of points to closest centroid)
ğ‘‘(ğ‘ğ‘–, ğ‘ğ‘—) = 1
ğ‘ğ‘– ğ‘ğ‘—
à·
ğ‘¥ğ‘–âˆˆğ¶ğ‘–
à·
ğ‘¥ğ‘—âˆˆğ¶ğ‘—
ğ‘‘(ğ‘¥ğ‘–, ğ‘¥ğ‘—)
--- Page 25 ---
25
ğ‘¥1
ğ‘¥3 
ğ‘¥5 
ğ‘¥4 
ğ‘¥2
â€¦
â‹®
Similarity?
similarity matrix
â–ª MIN (single link)
â–ª MAX (complete link)
â–ª Average link
â–ª Centroid link
â–ª Wardâ€™s method
Cluster distance
ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥4 ğ‘¥5 
--- Page 26 ---
26
ğ‘¥1
ğ‘¥3 
ğ‘¥5 
ğ‘¥4 
ğ‘¥2
â€¦
â‹®
similarity matrix
â–ª MIN (single link)
â–ª MAX (complete link)
â–ª Average link
â–ª Centroid link
â–ª Wardâ€™s method
Cluster distance
ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥4 ğ‘¥5 
--- Page 27 ---
27
ğ‘¥1
ğ‘¥3 
ğ‘¥5 
ğ‘¥4 
ğ‘¥2
â€¦
â‹®
similarity matrix
â–ª MIN (single link)
â–ª MAX (complete link)
â–ª Average link
â–ª Centroid link
â–ª Wardâ€™s method
Cluster distance
ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥4 ğ‘¥5 
--- Page 28 ---
28
ğ‘¥1
ğ‘¥3 
ğ‘¥5 
ğ‘¥4 
ğ‘¥2
â€¦
â‹®
similarity matrix
â–ª MIN (single link)
â–ª MAX (complete link)
â–ª Average link
â–ª Centroid link
â–ª Wardâ€™s method
Cluster distance
ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥4 ğ‘¥5 
--- Page 29 ---
29
ğ‘¥1
ğ‘¥3 
ğ‘¥5 
ğ‘¥4 
ğ‘¥2
â€¦
â‹®
similarity matrix
â–ª MIN (single link)
â–ª MAX (complete link)
â–ª Average link
â–ª Centroid link
â–ª Wardâ€™s method
Cluster distance
ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥4 ğ‘¥5 
ï‚´ ï‚´
--- Page 30 ---
30
â€¢ We begin with a distance matrix which contains the                                                                 
distances between every pair of objects in our database
Hierarchical clustering
0 8 8 7 7
0 2 4 4
0 3 3
0 1
0
d(   ,  ) = 8
d(   ,  ) = 1

--- Page 31 ---
31
â€¦
Consider all 
possible 
mergesâ€¦
Choose 
the best
Hierarchical clustering
Bottom-up (agglomerative): Starting with each point as 
a cluster, find best pair. Repeat until all clusters are fused
--- Page 32 ---
32
â€¦
Consider all 
possible 
mergesâ€¦
Choose 
the best
Consider all 
possible 
mergesâ€¦
â€¦
Choose 
the best
Hierarchical clustering
Bottom-up (agglomerative): Starting with each point as 
a cluster, find best pair. Repeat until all clusters are fused
--- Page 33 ---
33
â€¦
Consider all 
possible 
mergesâ€¦
Choose 
the best
Consider all 
possible 
mergesâ€¦
â€¦
Choose 
the best
Consider all 
possible 
mergesâ€¦
Choose 
the bestâ€¦
Hierarchical clustering
Bottom-up (agglomerative)
--- Page 34 ---
34
original points clusters
â€¢ Can handle non-elliptical shapes
MIN: strengths and limitations
â€¢Overlapping clusters and noise
original points clusters
--- Page 35 ---
35
Original Points
â€¢ Less susceptible to noise and outliers
MAX: strengths and limitations
â€¢ Tends to break large clusters
â€¢ Biased towards globular clusters
original points clusters
original points clusters
--- Page 36 ---
36
Average
Wardâ€™s 1
2
3
4
5
6
1
2
5
3
4
MIN
MAX
1
2
3
4
5
6
1
2
5
3
4
1
2
3
4
5
6
1
2 5
3
41
2
3
4
5
6
1
2
3
4
5
Hierachical clustering: comparison
â€¢ problems MIN and MAX link 
can be minimized under 
average/centroid/Ward link
â€¢ strength: less susceptible 
to noise and outliers
â€¢ limitation: biases towards 
globular clusters
--- Page 37 ---
37
DBSCAN (density-based clustering)
â€¢ clusters are defined as areas of higher density
â€¢ separation occurs in sparse areas                                                       
â€¢ isolated data points here seen as outliers
â€¢ advantages? limitations?

--- Page 38 ---
38
â€¢ parameters
â€¢ ï¥ maximum distance
â€¢ p minimum neighbors
â€¢ algorithm
â€¢ for each point: 
         cluster points with p 
         neighbors at < ï¥ distance 
DBSCAN (density-based clustering)
--- Page 39 ---
39
Outline
â€¢ Introduction to clustering
â€¢ Multivariate similarity metrics
â€¢ Approaches
â€¢ hierarchical
â€¢ density-based
â€¢ From multivariate to complex data structures
â€¢ Evaluation
â€¢ intrinsic metrics
â€¢ extrinsic metrics
--- Page 40 ---
40
Time series data
â€¢ Time series: sequence of values or symbols along time ğ¬ =< ğ±1, . . , ğ±ğ‘‡ >
â€¢ univariate or multivariate, ğ±ğ‘— âˆˆ â„ğ‘š (or ğ±ğ‘— âˆˆ {ğ‘Œ1. . ğ‘Œğ‘š}), where ğ‘š is the multivariate order
â€¢ Time series data: {ğ¬1, . . , ğ¬ğ‘›} where ğ¬ğ‘– is a time series
â€¢ Time series are ubiquotous:                                                                                                                    
monitoring biological, individual, organizational, geophysical, digital, mechanical, societal systems
â€¢ Movement, image and video as time series, text data as time series
â€¢ People measure things...
â€¢ their blood pressure
â€¢ the annual rainfall in New Zealand
â€¢ the value of their Yahoo stock
â€¢ the number of web hits per second
 â€¦ and things change over time
--- Page 41 ---
41
Time series clustering

--- Page 42 ---
42
Text document clustering
â€¢ Group related documents based on their content
â€¢ the similarity between every string pair is calculated as a basis for determining the clusters
â€¢ considering term vector spacesâ€¦ cosine
â€¢ A similarity measure is required to calculate the similarity between two strings
approximate   
string matching
semantic similarity
stem, feature extraction and lexical analysis
D1
D2
sports
T1 T2 â€¦.â€¦â€¦  Tm
12   0    â€¦.â€¦â€¦   6
Dn
class
travel
jobs
â€¦
â€¦
thousands of terms
documents
3   10   â€¦.â€¦â€¦  28
0   11   â€¦.â€¦â€¦  16
â€¦
--- Page 43 ---
43
GPS trajectory clustering

--- Page 44 ---
44
Homes 
of usersShop
Walking 
street
Market
place
Swim
hall
Science
park
Spatial clustering

--- Page 45 ---
45
Image and video clustering
Image: Gansbeke et al.
Image:  Liu and Shah

--- Page 46 ---
46
Representation learning (next class)
These and many other complex data structures may be encountered
â€¢ video, events, tensors, heterogeneous data structuresâ€¦
Two major solutions
â€¢ dedicated distances or clustering approaches
â€¢ obtain (numeric) representations of these complex observations by extracting features
â€¢ features can be extracted using simple statistics
â€¢ e.g. extract centrality/variability/slope/max/min statistics on time series using sliding windows
â€¢ embeddings can be extracted using representation learning
â€¢ example: auto-encoder neural networks can                                                                                   
be applied to deal with arbitrary complex inputs
--- Page 47 ---
47
Outline
â€¢ Introduction to clustering
â€¢ Multivariate similarity metrics
â€¢ Approaches
â€¢ hierarchical
â€¢ density-based
â€¢ From multivariate to complex data structures
â€¢ Evaluation
â€¢ intrinsic metrics
â€¢ extrinsic metrics
--- Page 48 ---
48
Evaluation: clustering quality
â€¢ 3 kinds of measures: external, internal and relative indexes
â€¢ External (supervised): extent to which cluster labels match true labels
â€¢ requires prior or expert knowledge
â€¢ Internal (unsupervised): goodness without external information
â€¢ how well they are separated (e.g. silhouette)
â€¢ should be independent from algorithm-specific functions (unbiased)
â€¢ Relative: compare different cluster analyses (different parameters/algorithms)
--- Page 49 ---
49
Proximity graph-based approach to measure cohesion and separation
â€¢ Cohesion is the sum of the weight of all links within a cluster
â€¢ Separation is the sum of the weights between nodes in the cluster and nodes outside the cluster
cohesion separation
Internal measures: cohesion and separation
--- Page 50 ---
50
â€¢ Cohesion (e.g. sum of squared errors or sum of square within): 
           how closely related are points in a cluster
â€¢ Separation (e.g. sum of squares between clusters)
             how distinct or well-separated a cluster is from other clusters
â€¢ Total error (e.g. sum of squares): within and between errors
Internal measures: cohesion and separation
ğ‘†ğ‘†ğµ = ğµğ‘†ğ‘† = à·
ğ‘˜
ğ‘ğ‘˜ ğ‘‘(ğ‘ğ‘˜, Ç‰ğ‘¥)2
ğ‘†ğ‘†ğ¸ = ğ‘†ğ‘†ğ‘Š = à·
ğ‘˜=1
ğ¾
à·
ğ‘¥ğ‘–âˆˆğ¶ğ‘˜
ğ‘‘(ğ‘¥ğ‘–, ğ‘ğ‘˜)2
ğ‘‡ğ‘†ğ‘† = ğ‘†ğ‘†ğµ + ğ‘†ğ‘†ğ¸
ğ‘‡ğ‘†ğ‘† = à·
ğ‘–
ğ‘›
ğ‘‘(ğ‘¥ğ‘–, Ç‰ğ‘¥)2
--- Page 51 ---
51
Internal measures: cohesion and separation
SSB + SSE = constant
1 2 3 4 5
ï‚´ ï‚´ï‚´
m1 m2
m
ğ‘†ğ‘†ğ¸ = (1 âˆ’ 1.5)2 + (2 âˆ’ 1.5)2 + (4 âˆ’ 4.5)2 + (5 âˆ’ 4.5)2 = 1
ğ‘†ğ‘†ğµ = 2 Ã— (3 âˆ’ 1.5)2 + 2 Ã— (4.5 âˆ’ 3)2 = 9
ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ = 1 + 9 = 10
K=2 clusters:
ğ‘†ğ‘†ğ¸ = (1 âˆ’ 3)2 + (2 âˆ’ 3)2 + (4 âˆ’ 3)2 + (5 âˆ’ 3)2 = 10
ğ‘†ğ‘†ğµ = 4 Ã— (3 âˆ’ 3)2 = 0
ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ = 10 + 0 = 10
K=1 cluster:
--- Page 52 ---
52
â€¢ For each observation, the error is the distance to the nearest cluster
â€¢ Square these errors (to penalize larger distances) and sum these errors
â€¢ Good to compare two clustering solutions or two clusters
â€¢ Can also be used to estimate the number of clusters
ğ‘†ğ‘†ğ¸ = à·
ğ‘˜=1
ğ¾
à·
ğ‘¥ğ‘–âˆˆğ¶ğ‘˜
ğ‘‘(ğ‘¥ğ‘–, ğ‘ğ‘˜)2
2 5 10 15 20 25 300
1
2
3
4
5
6
7
8
9
10
K
SSE
5 10 15
-6
-4
-2
0
2
4
6
Internal measures: cohesion
--- Page 53 ---
53
Internal measures: cohesion
Challenge on finding optimal #clusters:
â€¢ an easy way to reduce SSE is to increase the #clusters
â€¢ solution: elbow method (next class)
1 
2
3
5
6
4
7
SSE of clusters found using K-means

--- Page 54 ---
54
â€¢ Combine ideas of both cohesion and separation
â€¢ Calculated for a specific object ğ±ğ‘–
â€¢ ğ‘ = average distance of ğ±ğ‘– to the points in its cluster
â€¢ ğ‘ = min (average distance of ğ±ğ‘– to points in another cluster)
â€¢ the silhouette coefficient for a point is then given by 
ğ‘  = 1â€“ ğ‘/ğ‘   if ğ‘ < ğ‘,   (or ğ‘  = ğ‘/ğ‘ âˆ’ 1 if ğ‘ ï‚³ ğ‘, not the usual case) 
between âˆ’1 and 1 (the closer to 1 the better)
â€¢ Silhouette of cluster and clustering solution: average of silhouettes
a
b
Internal measures: silhouette coefficient

--- Page 55 ---
55
Internal measures: silhouette coefficient

--- Page 56 ---
56
â€¢  Order the similarity matrix with respect to cluster labels and inspect visually
0 0.2 0.4 0.6 0.8 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
y
Points
Points
20 40 60 80 100
10
20
30
40
50
60
70
80
90
100
Similarity
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Internal measures: similarity matrix
--- Page 57 ---
57
Points
Points
20 40 60 80 100
10
20
30
40
50
60
70
80
90
100
Similarity
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
k-means
0 0.2 0.4 0.6 0.8 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
y
Internal measures: similarity matrix
â€¢  Clusters in random data are not well-defined
Points
Points
20 40 60 80 100
10
20
30
40
50
60
70
80
90
100
Similarity
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
complete link
--- Page 58 ---
58
Recall: clustering evaluation
â€¢ 3 kinds of measures: external, internal and relative indexes
â€¢ External (supervised): extent to which cluster labels match true labels
â€¢ requires prior or expert knowledge
â€¢ Internal (unsupervised): goodness without external information
â€¢ how well they are separated (e.g. silhouette)
â€¢ should be independent from algorithm-specific functions (unbiased)
â€¢ Relative: compare different cluster analyses (different parameters/algorithms)
--- Page 59 ---
59
External measures: purity
â€¢ â„¦ =  {ğœ”1, ğœ”2, . . . , ğœ”ğ¾} is the set of clusters
 ğ¶ = {ğ‘1, ğ‘2, . . . , ğ‘ğ½} is the set of classes
â€¢ For each cluster ğœ”ğ‘˜: find class ğ‘ğ‘— with most objects in ğœ”ğ‘˜, ğ‘›ğ‘˜ğ‘— 
â€¢ Sum all ğ‘›ğ‘˜ğ‘— and divide by total number of points
â€¢ Problem: biased â‡’ n clusters maximizes purity
â€¢ Alternatives: entropy of classes in clusters
ğ‘ğ‘¢ğ‘Ÿğ‘–ğ‘¡ğ‘¦(Î©, ğ¶) = 1
ğ‘› à·
ğ‘˜
max
ğ‘—
|ğœ”ğ‘˜ âˆ© ğ‘ğ‘—|
--- Page 60 ---
60
â€¢         â€¢
     â€¢   â€¢
     â€¢  â€¢
â€¢         â€¢
â€¢   â€¢
     â€¢  â€¢
â€¢         â€¢
     â€¢   â€¢
       â€¢
cluster I cluster II cluster III
cluster I: purity = 1/6 (max(5, 1, 0)) = 5/6
cluster II: purity = 1/6 (max(1, 4, 1)) = 4/6
cluster III: purity = 1/5 (max(2, 0, 3)) = 3/5
Sec. 16.3
solution: purity = 1/17 (5+4+3) = 12/17
External measures: purity
--- Page 61 ---
61
â€¢ Counts of object pairs
â€¢ Rand index  RI =
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘+ğ‘‡ğ‘
â€¢ Given a specific cluster (positive):
â€¢ precision = TP/(TP+FP)
â€¢ recall = TP/(TP+FN)
â€¢ F-measure = 2ï‚´precisionï‚´recall / (precision + recall)
External measures: rand index
same cluster            different clusters
         same class       true positives (TP)    false negatives (FN)
different classes      false positives (FP)    true negatives (TN)
--- Page 62 ---
Number of 
object pairs Same cluster Different clusters
Same class in 
ground truth 20 24
Different classes 
in ground truth 20 72
Sec. 16.3
External measures: rand index
Rand index? 
--- Page 63 ---
63
Thank you!
Rui Henriques
rmch@tecnico.ulisboa.pt 
