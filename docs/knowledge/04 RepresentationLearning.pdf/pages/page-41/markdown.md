# CNNs, RNNs, self-attention

- convolutions and recurrence replace full connections with different forms of locality
- self-attention rely on flexible positional encodings (not requiring processing in any fixed order)

![img-44.jpeg](img-44.jpeg)

![img-45.jpeg](img-45.jpeg)

![img-46.jpeg](img-46.jpeg)

TÉCNICO+

FORMAÇÃO AVANÇADA

https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html