# Hyperparameterization

- How many layers? Some of the mentioned hyperparameters:
- #layers, #neurons per layer, activations
- loss function, learning rate, batch size
- others: layering (more to come!), regularization (e.g. penalty, dropout rate), momentum, decay...

- The hyperparametric choices define the **architecture** of the neural network

- So many choices ☹ How do we select the best hyperparameters?
- **manual** optimization (rely on *intuition* yet discouraged as DL is not rocket science)
- **automatic** optimization can be exhaustive (grid searches) or approximate (default option)
- many good packages available for effective approximate optimization (e.g. optuna)

TÉCNICO+ FORMAÇÃO AVANÇADA