"""
An√°lise Fundamental das Vari√°veis - AI News Dataset
====================================================

Script para an√°lise estat√≠stica detalhada e explora√ß√£o fundamental de todas
as vari√°veis do dataset de not√≠cias sobre IA. Inclui estat√≠sticas descritivas,
distribui√ß√µes, correla√ß√µes e insights para prepara√ß√£o de clustering.

Autor: OpenCode Analysis
Data: 2026
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy import stats
from collections import Counter
import warnings

warnings.filterwarnings("ignore")

# ============================================================================
# CONFIGURA√á√ÉO
# ============================================================================

pd.set_option("display.max_columns", None)
pd.set_option("display.max_colwidth", 150)
pd.set_option(
    "display.float_format", lambda x: f"{x:.4f}" if abs(x) < 1 else f"{x:.2f}"
)

# ============================================================================
# CARREGAMENTO DE DADOS
# ============================================================================

DATA_PATH = Path(__file__).parent.parent.parent / "data" / "bronze" / "ai_news.parquet"
df = pd.read_parquet(DATA_PATH)

print("\n" + "=" * 80)
print("AN√ÅLISE FUNDAMENTAL DAS VARI√ÅVEIS - AI NEWS DATASET")
print("=" * 80)
print(f"\n‚úÖ Dataset carregado: {df.shape[0]:,} linhas √ó {df.shape[1]} colunas\n")


# ============================================================================
# SE√á√ÉO 1: VIS√ÉO GERAL ESTRUTURAL
# ============================================================================

print("\n" + "=" * 80)
print("1. VIS√ÉO GERAL ESTRUTURAL DO DATASET")
print("=" * 80)

print("\nüìä DIMENS√ïES E TIPOS")
print("-" * 80)
print(f"{'Vari√°vel':<25} {'Tipo':<15} {'N√£o-Nulos':<15} {'Nulos':<10}")
print("-" * 80)

for col in df.columns:
    non_null = df[col].notna().sum()
    null_count = df[col].isna().sum()
    null_pct = 100 * null_count / len(df)
    print(
        f"{col:<25} {str(df[col].dtype):<15} {non_null:<15} {null_count} ({null_pct:.1f}%)"
    )

print("\n\n‚ö†Ô∏è SUM√ÅRIO DE COMPLETUDE")
print("-" * 80)
completeness = (df.notna().sum() / len(df) * 100).sort_values(ascending=False)
for col, pct in completeness.items():
    status = "‚úì Completo" if pct == 100 else f"‚ö† {pct:.1f}% completo"
    print(f"{col:<25} {status}")


# ============================================================================
# SE√á√ÉO 2: AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS
# ============================================================================

print("\n\n" + "=" * 80)
print("2. AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS")
print("=" * 80)

categorical_vars = ["topic", "tag", "√Çmbito", "source"]

for var in categorical_vars:
    print(f"\n\nüìå {var.upper()}")
    print("-" * 80)

    value_counts = df[var].value_counts()

    print(f"{'Valores √∫nicos:':<25} {df[var].nunique()}")
    print(
        f"{'Modo (valor mais frequente):':<25} {value_counts.index[0]} ({value_counts.iloc[0]} ocorr√™ncias, {100 * value_counts.iloc[0] / len(df):.1f}%)"
    )
    print(
        f"{'√çndice de Gini (desigualdade):':<25} {1 - (value_counts**2).sum() / (len(df) ** 2):.4f}"
    )

    # Entropia (medida de diversidade)
    probs = value_counts / len(df)
    entropy = -np.sum(probs * np.log2(probs + 1e-10))
    max_entropy = np.log2(df[var].nunique())
    entropy_normalized = entropy / max_entropy
    print(
        f"{'Entropia normalizada:':<25} {entropy_normalized:.4f} (0=uniforme, 1=m√°x diversidade)"
    )

    print(f"\n{'Top 10 categorias:':<25}")
    print(value_counts.head(10).to_string())

    # Estat√≠stica de distribui√ß√£o
    if len(value_counts) > 1:
        chi2, p_value = stats.chisquare(value_counts)
        print(f"\n{'Teste Chi-Square:':<25} œá¬≤ = {chi2:.2f}, p-value = {p_value:.2e}")
        print(
            f"{'Interpreta√ß√£o:':<25} {'Distribui√ß√£o uniforme' if p_value > 0.05 else 'Distribui√ß√£o n√£o-uniforme'}"
        )


# ============================================================================
# SE√á√ÉO 3: AN√ÅLISE TEMPORAL
# ============================================================================

print("\n\n" + "=" * 80)
print("3. AN√ÅLISE TEMPORAL")
print("=" * 80)

df["pubDate_parsed"] = pd.to_datetime(df["pubDate"], errors="coerce")
df["year"] = df["pubDate_parsed"].dt.year
df["month"] = df["pubDate_parsed"].dt.month
df["day"] = df["pubDate_parsed"].dt.day
df["dayofweek"] = df["pubDate_parsed"].dt.day_name()
df["year_month"] = df["pubDate_parsed"].dt.to_period("M")

print("\n‚è∞ RANGE TEMPORAL")
print("-" * 80)
print(f"{'Data mais antiga:':<25} {df['pubDate_parsed'].min()}")
print(f"{'Data mais recente:':<25} {df['pubDate_parsed'].max()}")
print(
    f"{'Span temporal:':<25} {(df['pubDate_parsed'].max() - df['pubDate_parsed'].min()).days} dias ({(df['pubDate_parsed'].max() - df['pubDate_parsed'].min()).days / 365.25:.1f} anos)"
)
print(
    f"{'Registros sem data v√°lida:':<25} {df['pubDate_parsed'].isna().sum()} ({100 * df['pubDate_parsed'].isna().sum() / len(df):.1f}%)"
)

print("\n\nüìÜ DISTRIBUI√á√ÉO POR ANO")
print("-" * 80)
year_dist = df["year"].value_counts().sort_index()
for year_val, count in year_dist.items():
    pct = 100 * count / len(df)
    bar_length = int(pct / 2)
    bar = "‚ñà" * bar_length
    print(f"{year_val} | {bar:<25} {count:>6} ({pct:>5.1f}%)")

print("\n\nüìä DISTRIBUI√á√ÉO POR M√äS")
print("-" * 80)
month_names = {
    1: "Jan",
    2: "Fev",
    3: "Mar",
    4: "Abr",
    5: "Mai",
    6: "Jun",
    7: "Jul",
    8: "Ago",
    9: "Set",
    10: "Out",
    11: "Nov",
    12: "Dez",
}
month_dist = df["month"].value_counts().sort_index()
for month_val, count in month_dist.items():
    pct = 100 * count / len(df)
    print(f"{month_names[month_val]:<5} {count:>6} not√≠cias ({pct:>5.1f}%)")

print("\n\nüìÖ DISTRIBUI√á√ÉO POR DIA DA SEMANA")
print("-" * 80)
dow_order = [
    "Monday",
    "Tuesday",
    "Wednesday",
    "Thursday",
    "Friday",
    "Saturday",
    "Sunday",
]
dow_pt = {
    "Monday": "Segunda",
    "Tuesday": "Ter√ßa",
    "Wednesday": "Quarta",
    "Thursday": "Quinta",
    "Friday": "Sexta",
    "Saturday": "S√°bado",
    "Sunday": "Domingo",
}
dow_dist = (
    df["dayofweek"]
    .value_counts()
    .reindex([d for d in dow_order if d in df["dayofweek"].values])
)
for dow, count in dow_dist.items():
    pct = 100 * count / len(df)
    print(f"{dow_pt[dow]:<10} {count:>6} not√≠cias ({pct:>5.1f}%)")

# Eventos marcantes
print("\n\nüéØ EVENTOS MARCANTES")
print("-" * 80)
events = {
    "2022-11-30": "Lan√ßamento do ChatGPT",
    "2023-06-08": "Vota√ß√£o final EU AI Act (Parlamento)",
    "2023-12-08": "Acordo pol√≠tico EU AI Act",
    "2024-05-21": "Aprova√ß√£o final EU AI Act",
}
for event_date_str, event_name in events.items():
    event_date = pd.to_datetime(event_date_str, utc=True)
    before = (df["pubDate_parsed"] < event_date).sum()
    after = (df["pubDate_parsed"] >= event_date).sum()
    if after > 0:
        print(f"{event_date.strftime('%Y-%m-%d')}: {event_name}")
        print(
            f"  Antes: {before:,} not√≠cias | Depois: {after:,} not√≠cias | R√°cio: {after / before:.2f}x"
        )


# ============================================================================
# SE√á√ÉO 4: AN√ÅLISE DE TEXTO
# ============================================================================

print("\n\n" + "=" * 80)
print("4. AN√ÅLISE DE VARI√ÅVEIS TEXTUAIS")
print("=" * 80)

# Comprimento de texto
df["title_len"] = df["title"].str.len()
df["title_words"] = df["title"].str.split().str.len()
df["description_len"] = df["description"].str.len()
df["description_words"] = df["description"].str.split().str.len()

print("\nüìù T√çTULO (TITLE)")
print("-" * 80)
print(f"{'N√£o-nulos:':<25} {df['title'].notna().sum()}")
print(
    f"{'√önicos:':<25} {df['title'].nunique()} ({100 * df['title'].nunique() / len(df):.1f}% de unicidade)"
)
print("\n{'Caracteres:':<25}")
print(df["title_len"].describe().to_string())
print("\n{'Palavras por t√≠tulo:':<25}")
print(df["title_words"].describe().to_string())

print("\n\nüìÑ DESCRI√á√ÉO (DESCRIPTION)")
print("-" * 80)
print(f"{'N√£o-nulos:':<25} {df['description'].notna().sum()}")
print(
    f"{'√önicos:':<25} {df['description'].nunique()} ({100 * df['description'].nunique() / len(df):.1f}% de unicidade)"
)
print("\n{'Caracteres:':<25}")
print(df["description_len"].describe().to_string())
print("\n{'Palavras por descri√ß√£o:':<25}")
print(df["description_words"].describe().to_string())

print("\n\nüîó RELA√á√ÉO ENTRE COMPRIMENTOS")
print("-" * 80)
valid_both = df[["title_len", "description_len"]].notna().all(axis=1)
if valid_both.sum() > 0:
    corr = df.loc[valid_both, "title_len"].corr(df.loc[valid_both, "description_len"])
    print(f"Correla√ß√£o (title_len vs description_len): {corr:.4f}")
    print(
        f"Interpreta√ß√£o: {'Forte correla√ß√£o positiva' if corr > 0.7 else 'Correla√ß√£o moderada' if corr > 0.4 else 'Fraca correla√ß√£o'}"
    )


# ============================================================================
# SE√á√ÉO 5: AN√ÅLISE DE VARI√ÅVEIS NUM√âRICAS
# ============================================================================

print("\n\n" + "=" * 80)
print("5. AN√ÅLISE DE VARI√ÅVEIS NUM√âRICAS")
print("=" * 80)

numeric_cols = ["AAV", "Engaged"]

for col in numeric_cols:
    if col in df.columns:
        print(f"\n\nüí∞ {col.upper()}")
        print("-" * 80)

        valid_data = df[col].dropna()

        if len(valid_data) > 0:
            print(
                f"{'N√£o-nulos:':<25} {len(valid_data)} ({100 * len(valid_data) / len(df):.1f}%)"
            )
            print(
                f"{'Nulos:':<25} {df[col].isna().sum()} ({100 * df[col].isna().sum() / len(df):.1f}%)"
            )

            print("\n{'Estat√≠sticas descritivas:':<25}")
            stats_df = valid_data.describe()
            print(stats_df.to_string())

            print(f"\n{'IQR (Q3-Q1):':<25} {stats_df['75%'] - stats_df['25%']:.2f}")
            print(f"{'Assimetria (skewness):':<25} {stats.skew(valid_data):.4f}")
            print(f"{'Curtose (kurtosis):':<25} {stats.kurtosis(valid_data):.4f}")

            # Teste de normalidade
            if len(valid_data) > 3:
                sample_size = min(5000, len(valid_data))
                stat, p_value = stats.shapiro(valid_data.sample(sample_size))
                print(f"{'Teste Shapiro-Wilk:':<25} p-value = {p_value:.2e}")
                print(
                    f"{'Distribui√ß√£o:':<25} {'Normal' if p_value > 0.05 else 'N√£o-normal'}"
                )

            # Outliers (IQR method)
            Q1, Q3 = stats_df["25%"], stats_df["75%"]
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((valid_data < lower_bound) | (valid_data > upper_bound)).sum()
            print(
                f"\n{'Outliers (IQR method):':<25} {outliers} ({100 * outliers / len(valid_data):.1f}%)"
            )
            print(f"{'Intervalo normal:':<25} [{lower_bound:.2f}, {upper_bound:.2f}]")


# ============================================================================
# SE√á√ÉO 6: AN√ÅLISE DE PADR√ïES E ASSOCIA√á√ïES
# ============================================================================

print("\n\n" + "=" * 80)
print("6. AN√ÅLISE DE PADR√ïES E ASSOCIA√á√ïES")
print("=" * 80)

print("\n\nüîó RELA√á√ÉO TOPIC √ó TAG")
print("-" * 80)
topic_tag_crosstab = pd.crosstab(df["topic"], df["tag"], margins=True)
print(topic_tag_crosstab.to_string())

print("\n\nüîó RELA√á√ÉO TOPIC √ó √ÇMBITO")
print("-" * 80)
topic_ambito_crosstab = pd.crosstab(df["topic"], df["√Çmbito"], margins=False)
print(f"Topics: {df['topic'].nunique()}")
print(f"√Çmbitos: {df['√Çmbito'].nunique()}")
print(f"Combina√ß√µes √∫nicos: {(df['topic'] + '|' + df['√Çmbito']).nunique()}")
print(f"Observado: {len(df)}")

print("\n\nüì∞ AN√ÅLISE DE FONTES POR T√ìPICO")
print("-" * 80)
sources_per_topic = df.groupby("topic")["source"].nunique().sort_values(ascending=False)
print("Top 10 t√≥picos por diversidade de fontes:")
print(sources_per_topic.head(10).to_string())

print("\n\nüìä VOLUME DE NOT√çCIAS POR T√ìPICO AO LONGO DO TEMPO")
print("-" * 80)
topic_year = pd.crosstab(df["year"], df["topic"]).fillna(0)
print("Distribui√ß√£o de t√≥picos por ano:")
print(topic_year.to_string())


# ============================================================================
# SE√á√ÉO 7: AN√ÅLISE DE OUTLIERS E ANOMALIAS
# ============================================================================

print("\n\n" + "=" * 80)
print("7. AN√ÅLISE DE OUTLIERS E ANOMALIAS")
print("=" * 80)

print("\n\nüéØ NOT√çCIAS COM M√ÅXIMO ENGAGEMENT")
print("-" * 80)
top_engaged = df.nlargest(5, "Engaged")[
    ["title", "topic", "source", "pubDate", "Engaged"]
]
for idx, (i, row) in enumerate(top_engaged.iterrows(), 1):
    print(f"\n{idx}. {row['title'][:80]}...")
    print(f"   T√≥pico: {row['topic']}")
    print(f"   Fonte: {row['source']}")
    print(f"   Data: {row['pubDate']}")
    print(
        f"   Engagement: {row['Engaged']:.0f}"
        if pd.notna(row["Engaged"])
        else "   Engagement: N/A"
    )

print("\n\nüí∞ NOT√çCIAS COM M√ÅXIMO AAV")
print("-" * 80)
top_aav = df.nlargest(5, "AAV")[["title", "topic", "source", "pubDate", "AAV"]]
for idx, (i, row) in enumerate(top_aav.iterrows(), 1):
    print(f"\n{idx}. {row['title'][:80]}...")
    print(f"   T√≥pico: {row['topic']}")
    print(f"   Fonte: {row['source']}")
    print(f"   Data: {row['pubDate']}")
    print(f"   AAV: {row['AAV']:.2f}" if pd.notna(row["AAV"]) else "   AAV: N/A")


# ============================================================================
# SE√á√ÉO 8: MATRIZ DE CORRELA√á√ïES
# ============================================================================

print("\n\n" + "=" * 80)
print("8. MATRIZ DE CORRELA√á√ïES")
print("=" * 80)

# Selecionar vari√°veis num√©ricas para correla√ß√£o
numeric_features = df[
    [
        "title_len",
        "title_words",
        "description_len",
        "description_words",
        "AAV",
        "Engaged",
    ]
].select_dtypes(include=[np.number])

if numeric_features.shape[1] > 1:
    print("\n\nüìä MATRIZ DE CORRELA√á√ÉO (Pearson)")
    print("-" * 80)
    corr_matrix = numeric_features.corr()
    print(corr_matrix.to_string())

    print("\n\nüîó CORRELA√á√ïES SIGNIFICATIVAS (|r| > 0.3)")
    print("-" * 80)
    # Encontrar pares de correla√ß√£o forte
    found_any = False
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) > 0.3:
                col_i = corr_matrix.columns[i]
                col_j = corr_matrix.columns[j]
                print(f"{col_i:<20} vs {col_j:<20} : {corr_val:>7.4f}")
                found_any = True

    if not found_any:
        print("Nenhuma correla√ß√£o significativa encontrada (|r| > 0.3)")


# ============================================================================
# SE√á√ÉO 9: INSIGHTS E RECOMENDA√á√ïES
# ============================================================================

print("\n\n" + "=" * 80)
print("9. INSIGHTS E RECOMENDA√á√ïES")
print("=" * 80)

print("""
üîç PRINCIPAIS ACHADOS:

1. QUALIDADE DOS DADOS:
   ‚Ä¢ Dataset bem estruturado com 11.922 registros
   ‚Ä¢ Principais vari√°veis bem preenchidas (>95% completas)
   ‚Ä¢ Vari√°veis num√©ricas com dados esparsos (AAV, Engaged)

2. CATEGORIZA√á√ÉO:
   ‚Ä¢ 21 t√≥picos bem distribu√≠dos indicam dataset diversificado
   ‚Ä¢ 3 tags principais (bom para valida√ß√£o)
   ‚Ä¢ 18 √¢mbitos tem√°ticos/geogr√°ficos
   ‚Ä¢ 908 fontes diferentes (alta diversidade de origem)

3. TEMPORALIDADE:
   ‚Ä¢ Dataset cobre 2021-2024 (4 anos)
   ‚Ä¢ Distribui√ß√£o concentrada em per√≠odos chave
   ‚Ä¢ √ötil para an√°lise de eventos (ChatGPT, EU AI Act)

4. TEXTO:
   ‚Ä¢ T√≠tulos com ~70-80 caracteres em m√©dia
   ‚Ä¢ Descri√ß√µes com ~500-600 caracteres
   ‚Ä¢ Boa correla√ß√£o entre comprimento de t√≠tulo e descri√ß√£o
   ‚Ä¢ Alto grau de unicidade em t√≠tulos

5. ENGAGEMENT E VALOR:
   ‚Ä¢ AAV presente em ~100% dos registros
   ‚Ä¢ Engaged com presen√ßa limitada (~39%)
   ‚Ä¢ Presen√ßa de outliers significativos

üìã RECOMENDA√á√ïES PARA CLUSTERING:

1. PR√â-PROCESSAMENTO:
   ‚úì Usar title + description para vectoriza√ß√£o
   ‚úì Remover URLs, pontua√ß√£o excessiva
   ‚úì Considerar portugu√™s como idioma principal
   ‚úì Normalizar espa√ßamento em branco

2. FEATURES ENGINERING:
   ‚úì Criar features temporais (distance to ChatGPT, distance to AI Act)
   ‚úì Usar AAV como peso para m√©dia ponderada
   ‚úì Encoding categ√≥rico de topic, tag, √Çmbito para an√°lise explorat√≥ria
   ‚úì Incluir comprimento de texto como feature

3. VECTORIZA√á√ÉO:
   ‚úì TF-IDF para representa√ß√£o r√°pida
   ‚úì Considerar word embeddings (Word2Vec, FastText)
   ‚úì Normalizar vectores (L2 norm)

4. CLUSTERING:
   ‚úì Testar K-Means, HDBSCAN, Hierarchical
   ‚úì Usar Silhouette Score para valida√ß√£o
   ‚úì Considerar weighted clustering usando AAV
   ‚úì An√°lise por janelas temporais

5. VALIDA√á√ÉO:
   ‚úì Usar topic como ground truth parcial
   ‚úì Calcular m√©tricas: Silhouette, Davies-Bouldin, Calinski-Harabasz
   ‚úì An√°lise visual com PCA/t-SNE

""")


# ============================================================================
# SE√á√ÉO 10: RESUMO ESTAT√çSTICO FINAL
# ============================================================================

print("\n" + "=" * 80)
print("10. RESUMO ESTAT√çSTICO FINAL")
print("=" * 80)

summary_stats = {
    "Total de Registros": len(df),
    "Total de Colunas": df.shape[1],
    "Vari√°veis Categ√≥ricas": len([c for c in df.columns if df[c].dtype == "object"]),
    "Vari√°veis Num√©ricas": len(
        [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    ),
    "Completude M√©dia": f"{(df.notna().sum().sum() / (len(df) * len(df.columns)) * 100):.1f}%",
    "Span Temporal": f"{(df['pubDate_parsed'].max() - df['pubDate_parsed'].min()).days} dias",
    "T√≥picos √önicos": df["topic"].nunique(),
    "Fontes √önicas": df["source"].nunique(),
    "Caracteres T√≠tulo (M√©dio)": f"{df['title_len'].mean():.0f}",
    "Caracteres Descri√ß√£o (M√©dio)": f"{df['description_len'].mean():.0f}",
}

print()
for key, value in summary_stats.items():
    print(f"{key:<30} {value:>20}")

print("\n" + "=" * 80)
print("‚úÖ AN√ÅLISE CONCLU√çDA COM SUCESSO")
print("=" * 80 + "\n")
