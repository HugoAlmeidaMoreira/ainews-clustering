@book{aggarwalOutlierAnalysis2017,
  title = {Outlier {{Analysis}}},
  author = {Aggarwal, Charu C.},
  date = {2017},
  publisher = {Springer},
  location = {Cham},
  abstract = {This book provides comprehensive coverage of the field of outlier analysis from a computer science point of view. It integrates methods from data mining, machine learning, and statistics within the computational framework and therefore appeals to multiple communities. The chapters of this book can be organized into three categories:Basic algorithms: Chapters 1 through 7 discuss the fundamental algorithms for outlier analysis, including probabilistic and statistical methods, linear methods, proximity-based methods, high-dimensional (subspace) methods, ensemble methods, and supervised methods.Domain-specific methods: Chapters 8 through 12 discuss outlier detection algorithms for various domains of data, such as text, categorical data, time-series data, discrete sequence data, spatial data, and network data.Applications: Chapter 13 is devoted to various applications of outlier analysis. Some guidance is also provided for the practitioner.The second edition of this book is more detailed and is written to appeal to both researchers and practitioners. Significant new material has been added on topics such as kernel methods, one-class support-vector machines, matrix factorization, neural networks, outlier ensembles, time-series methods, and subspace methods. It is written as a textbook and can be used for classroom teaching.},
  isbn = {978-3-319-47577-6},
  langid = {english},
  pagetotal = {488},
  file = {C:\Users\hugo\Zotero\storage\HELCLHF5\Aggarwal - 2017 - Outlier Analysis.pdf}
}

@online{bengioRepresentationLearningReview2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  date = {2014-04-23},
  eprint = {1206.5538},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1206.5538},
  url = {http://arxiv.org/abs/1206.5538},
  urldate = {2026-01-10},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\hugo\\Zotero\\storage\\Z2HQL8BT\\Bengio et al. - 2014 - Representation Learning A Review and New Perspectives.pdf;C\:\\Users\\hugo\\Zotero\\storage\\7RZBNUYG\\1206.html}
}

@misc{mcinnes2020umapuniformmanifoldapproximation,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03426}, 
}

@book{bishopDeepLearningFoundations2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  date = {2024},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urldate = {2026-01-17},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english}
}

@article{burkartSurveyExplainabilitySupervised2021,
  title = {A {{Survey}} on the {{Explainability}} of {{Supervised Machine Learning}}},
  author = {Burkart, Nadia and Huber, Marco F.},
  date = {2021-01-19},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {70},
  eprint = {2011.07876},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {245--317},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12228},
  url = {http://arxiv.org/abs/2011.07876},
  urldate = {2026-01-10},
  abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or fifinance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\hugo\\Zotero\\storage\\IPGJIDWI\\Burkart and Huber - 2021 - A Survey on the Explainability of Supervised Machine Learning.pdf;C\:\\Users\\hugo\\Zotero\\storage\\V2SFZFSG\\2011.html}
}

@article{henriquesTriclusteringAlgorithmsThreeDimensional2018,
  title = {Triclustering {{Algorithms}} for {{Three-Dimensional Data Analysis}}: {{A Comprehensive Survey}}},
  shorttitle = {Triclustering {{Algorithms}} for {{Three-Dimensional Data Analysis}}},
  author = {Henriques, Rui and Madeira, Sara C.},
  date = {2018-09-18},
  journaltitle = {ACM Comput. Surv.},
  volume = {51},
  number = {5},
  pages = {95:1--95:43},
  issn = {0360-0300},
  doi = {10.1145/3195833},
  url = {https://dl.acm.org/doi/10.1145/3195833},
  urldate = {2026-01-10},
  abstract = {Three-dimensional data are increasingly prevalent across biomedical and social domains. Notable examples are gene-sample-time, individual-feature-time, or node-node-time data, generally referred to as observation-attribute-context data. The unsupervised analysis of three-dimensional data can be pursued to discover putative biological modules, disease progression profiles, and communities of individuals with coherent behavior, among other patterns of interest. It is thus key to enhance the understanding of complex biological, individual, and societal systems. In this context, although clustering can be applied to group observations, its relevance is limited since observations in three-dimensional data domains are typically only meaningfully correlated on subspaces of the overall space. Biclustering tackles this challenge but disregards the third dimension. In this scenario, triclustering—the discovery of coherent subspaces within three-dimensional data—has been largely researched to tackle these problems. Despite the diversity of contributions in this field, there still lacks a structured view on the major requirements of triclustering, desirable forms of homogeneity (including coherency, structure, quality, locality, and orthonormality criteria), and algorithmic approaches. This work formalizes the triclustering task and its scope, introduces a taxonomy to categorize the contributions in the field, provides a comprehensive comparison of state-of-the-art triclustering algorithms according to their behavior and output, and lists relevant real-world applications. Finally, it highlights challenges and opportunities to advance the field of triclustering and its applicability to complex three-dimensional data analysis.},
  file = {C:\Users\hugo\Zotero\storage\R56GM4KZ\Henriques and Madeira - 2018 - Triclustering Algorithms for Three-Dimensional Data Analysis A Comprehensive Survey.pdf}
}

@online{LearningDeepRepresentations,
  title = {Learning {{Deep Representations}} of {{Data Distributions}}},
  url = {https://ma-lab-berkeley.github.io/deep-representation-learning-book/},
  urldate = {2026-01-10},
  file = {C:\Users\hugo\Zotero\storage\PN5ST5Y5\deep-representation-learning-book.html}
}

@book{zakiDataMiningMachine2020a,
  title = {Data {{Mining}} and {{Machine Learning}}: {{Fundamental Concepts}} and {{Algorithms}}},
  shorttitle = {Data {{Mining}} and {{Machine Learning}}},
  author = {Zaki, Mohammed J. and Jr, Wagner Meira},
  date = {2020},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  abstract = {The fundamental algorithms in data mining and machine learning form the basis of data science, utilizing automated methods to analyze patterns and models for all kinds of data in applications ranging from scientific discovery to business analytics. This textbook for senior undergraduate and graduate courses provides a comprehensive, in-depth overview of data mining, machine learning and statistics, offering solid guidance for students, researchers, and practitioners. The book lays the foundations of data analysis, pattern mining, clustering, classification and regression, with a focus on the algorithms and the underlying algebraic, geometric, and probabilistic concepts. New to this second edition is an entire part devoted to regression methods, including neural networks and deep learning.},
  isbn = {978-1-108-47398-9},
  langid = {english},
  pagetotal = {776},
  file = {C:\Users\hugo\Zotero\storage\256XI3J2\Zaki and Jr - 2020 - Data Mining and Machine Learning Fundamental Concepts and Algorithms.pdf}
}

@article{mcinnes2018umap,
  title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018},
  url={https://arxiv.org/abs/1802.03426}
}

@article{facco2017estimating,
   title={Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
   volume={7},
   ISSN={2045-2322},
   url={http://dx.doi.org/10.1038/s41598-017-11873-y},
   DOI={10.1038/s41598-017-11873-y},
   number={1},
   journal={Scientific Reports},
   publisher={Springer Science and Business Media LLC},
   author={Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
   year={2017}
}

@article{wallace2024meandifference,
  title={The Mean-Difference: A Simple and Effective Method for Zero-Shot Classification},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2403.14859},
  year={2024},
  url={https://arxiv.org/abs/2403.14859}
}

@inproceedings{xu2007scan,
  title={Scan: a structural clustering algorithm for networks},
  author={Xu, Xiaowei and Yuruk, Nurcan and Feng, Zhidan and Schweiger, Thomas AJ},
  booktitle={Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={824--833},
  year={2007}
}
